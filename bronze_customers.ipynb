{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0498307-91a6-4a2f-a865-0176bc97929b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType,StringType\n",
    "from pyspark.sql.functions import lit, current_timestamp,col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc24483b-ae7e-4ffb-b1ac-9781b2ef8daf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to process and write batches of data into a Delta table\n",
    "\n",
    "def process_batch(df, batch_id, table_name):\n",
    "    # Add a batch ID column to track processing\n",
    "    df = df.withColumn(\"batch_id\", lit(batch_id))\n",
    "    # Add a timestamp column for data ingestion time        \n",
    "    df = df.withColumn(\"ingest_datetime\", current_timestamp())\n",
    "    # Write the DataFrame to a Delta table in append mode\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "\n",
    "# Define the schema for customer data\n",
    "customer_schema = StructType([\n",
    "    StructField(\"CUSTOMER_ID\", IntegerType(), False),               # Unique identifier for each customer\n",
    "    StructField(\"CUST_FIRST_NAME\", StringType(), True),             # Customer's first name\n",
    "    StructField(\"CUST_LAST_NAME\", StringType(), True),              # Customer's last name\n",
    "    StructField(\"CUST_ADDRESS.COUNTRY_ID\", StringType(), True),     # Country ID in the address\n",
    "    StructField(\"CUST_ADDRESS.STATE_PROVINCE\", StringType(), True), # State or province in the address\n",
    "    StructField(\"CUST_ADDRESS.CITY\", StringType(), True),           # City in the address\n",
    "    StructField(\"CUST_ADDRESS.POSTAL_CODE\", StringType(), True),    # Postal code in the address\n",
    "    StructField(\"CUST_ADDRESS.STREET_ADDRESS\", StringType(), True), # Street address\n",
    "    StructField(\"PHONE_NUMBER\", StringType(), True),                # Customer's phone number\n",
    "    StructField(\"CUST_EMAIL\", StringType(), True),                  # Customer's email address\n",
    "    StructField(\"ACCOUNT_MGR_ID\", StringType(), True),              # Account manager ID\n",
    "    StructField(\"DATE_OF_BIRTH\", StringType(), True),               # Customer's date of birth\n",
    "    StructField(\"MARITAL_STATUS\", StringType(), True),              # Marital status\n",
    "    StructField(\"GENDER\", StringType(), True) ])                    # Gender\n",
    "\n",
    "\n",
    "# Specify table name, data source location, and checkpoint directory\n",
    "\n",
    "customer_table_name = 'stoyan.bronze_customers'\n",
    "customer_load_location = 's3://data-engineering-upskill-final-exam/stoyan/input_data/customers'\n",
    "customer_checkpoint_location = 's3://data-engineering-upskill-final-exam/stoyan/bronze_customers/checkpoint'\n",
    "\n",
    "# Define a streaming DataFrame for reading customer data\n",
    "\n",
    "customer_stream = (\n",
    "spark.readStream                \n",
    ".format(\"cloudFiles\")                                               # Enable Auto Loader for cloud file sources\n",
    ".option(\"wholeFile\", True)                                          # Treat the entire file as a single input\n",
    ".option(\"multiline\",True)                                           # Support multi-line CSV rows\n",
    ".option(\"cloudFiles.format\", \"csv\")                                 # Specify the input file format\n",
    ".option(\"header\", \"true\")                                           # Indicate that the first row contains headers\n",
    ".schema(customer_schema)                                            # Apply the defined schema\n",
    ".load(customer_load_location)                                       # Load data from the specified S3 location\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e1caaf5-57c4-466b-bd7f-4689f9e3c628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7febccdca990>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the stream to a Delta table with batch processing\n",
    "\n",
    "customer_stream.writeStream \\\n",
    ".foreachBatch(lambda df, batch_id: process_batch(df, batch_id, customer_table_name)) \\\n",
    ".option(\"checkpointLocation\", customer_checkpoint_location) \\\n",
    ".trigger(availableNow=True) \\\n",
    ".outputMode(\"append\") \\\n",
    ".start() "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_customers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}