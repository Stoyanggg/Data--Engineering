{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c2e618-e163-44c0-9558-c7223cf59b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType,StringType\n",
    "from pyspark.sql.functions import lit, current_timestamp,col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7512f7e6-1d14-44d6-a920-31b8d1543e2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to process and write batches of data into a Delta table\n",
    "\n",
    "def process_batch(df, batch_id, table_name):\n",
    "    # Add a batch ID column to identify the batch being processed\n",
    "    df = df.withColumn(\"batch_id\", lit(batch_id))\n",
    "    # Add a timestamp column to record the data ingestion time\n",
    "    df = df.withColumn(\"ingest_datetime\", current_timestamp())\n",
    "    # Write the DataFrame to a Delta table in append mode\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "\n",
    "# Define the S3 location for the \"products\" data and checkpoint directory\n",
    "products_load_location = 's3://data-engineering-upskill-final-exam/stoyan/input_data/products'\n",
    "products_checkpoint_location = 's3://data-engineering-upskill-final-exam/stoyan/bronze_products/checkpoint'\n",
    "products_table_name = 'stoyan.bronze_products'\n",
    "\n",
    "\n",
    "# Define the schema for the \"products\" data\n",
    "products_schema = StructType([\n",
    "    StructField(\"PRODUCT_ID\", IntegerType(), False),                # Unique identifier for each product\n",
    "    StructField(\"PRODUCT_NAME\", StringType(), True),                # Name of the product\n",
    "    StructField(\"CATEGORY_NAME\", StringType(), True),               # Category to which the product belongs\n",
    "    StructField(\"WEIGHT_CLASS\", IntegerType(), True),               # Weight classification of the product\n",
    "    StructField(\"PRODUCT_STATUS\", StringType(), True),              # Status of the product\n",
    "    StructField(\"LIST_PRICE\", DoubleType(), True),                  # Listed price of the product\n",
    "    StructField(\"MIN_PRICE\", DoubleType(), True)                    # Minimum allowable price for the product\n",
    "])\n",
    "\n",
    "\n",
    "# Define a streaming DataFrame for reading \"products\" data\n",
    "\n",
    "products_stream = (\n",
    "spark.readStream\n",
    ".format(\"cloudFiles\")                                               # Enable Auto Loader for cloud file sources\n",
    ".option(\"cloudFiles.format\", \"csv\")                                 # Specify the input file format as CSV\n",
    ".option(\"header\", \"true\")                                           # Indicate that the first row contains column headers\n",
    ".schema(products_schema)                                            # Apply the defined schema for products\n",
    ".load(products_load_location)                                       # Load data from the specified S3 location\n",
    ")\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02be4adf-e384-4fd4-8069-eb889a4d5c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7f3894754350>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "products_stream.writeStream \\\n",
    ".foreachBatch(lambda df, batch_id: process_batch(df, batch_id, products_table_name)) \\\n",
    ".option(\"checkpointLocation\", products_checkpoint_location) \\\n",
    ".trigger(availableNow=True) \\\n",
    ".outputMode(\"append\") \\\n",
    ".start() "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_products",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}