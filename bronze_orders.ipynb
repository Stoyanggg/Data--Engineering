{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "276d5ac9-35bb-4abf-86a2-c941272c38e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType,StringType,DateType\n",
    "from pyspark.sql.functions import lit, current_timestamp,col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223eda59-5d1d-4b9b-b769-4a1ccc179494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('drop table stoyan.bronze_orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a52b951-f54e-472b-be37-7f3b83a1fcaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to process and write batches of data into a Delta table\n",
    "\n",
    "def process_batch(df, batch_id, table_name):\n",
    "    # Add a batch ID column to identify the batch being processed\n",
    "    df = df.withColumn(\"batch_id\", lit(batch_id))\n",
    "    # Add a timestamp column to record the data ingestion time\n",
    "    df = df.withColumn(\"ingest_datetime\", current_timestamp())\n",
    "    # Write the DataFrame to a Delta table in append mode\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "# Define the schema for the \"orders\" data\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"ORDER_ID\", IntegerType(), False),                      # Unique identifier for each order\n",
    "    StructField(\"ORDER_DATE\", StringType(), True),                      # Date of the order\n",
    "    StructField(\"ORDER_MODE\", StringType(), True),                      # Mode of the order\n",
    "    StructField(\"CUSTOMER_ID\", IntegerType(), True),                    # Identifier for the customer placing the order\n",
    "    StructField(\"ORDER_STATUS\", IntegerType(), True),                   # Status of the order\n",
    "    StructField(\"ORDER_TOTAL\", DoubleType(), True),                     # Total value of the order\n",
    "    StructField(\"SALES_REP_ID\", IntegerType(), True),                   # Identifier for the sales representative\n",
    "    StructField(\"PROMOTION_ID\", StringType(), True)                     # Identifier for any applied promotions\n",
    "])\n",
    "\n",
    "# Specify table name, data source location, and checkpoint directory for \"orders\" data\n",
    "\n",
    "orders_table_name = 'stoyan.bronze_orders'\n",
    "orders_load_location = 's3://data-engineering-upskill-final-exam/stoyan/input_data/orders'\n",
    "orders_checkpoint_location = 's3://data-engineering-upskill-final-exam/stoyan/bronze_orders/checkpoint'\n",
    "\n",
    "\n",
    "# Define a streaming DataFrame for reading \"orders\" data\n",
    "\n",
    "orders_stream = (\n",
    "spark.readStream\n",
    ".format(\"cloudFiles\")                                                # Enable Auto Loader for cloud file sources\n",
    ".option(\"cloudFiles.format\", \"csv\")                                  # Specify the input file format as CSV\n",
    ".option(\"header\", \"true\")                                            # Indicate that the first row contains column headers\n",
    ".schema(orders_schema)                                               # Apply the defined schema for orders\n",
    ".load(orders_load_location)                                          # Load data from the specified S3 location\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efdec75e-e329-4c2a-a9e9-a02d4438a802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7fc7901d3c50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_stream.writeStream \\\n",
    ".foreachBatch(lambda df, batch_id: process_batch(df, batch_id, orders_table_name)) \\\n",
    ".option(\"checkpointLocation\", orders_checkpoint_location) \\\n",
    ".trigger(availableNow=True) \\\n",
    ".outputMode(\"append\") \\\n",
    ".start()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_orders",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}