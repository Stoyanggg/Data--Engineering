{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c93cfea-28ff-48b6-a811-f5ab7b9aae9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp,col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType,StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b97b7a-f511-42b5-a7c8-15fef7101aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to process and write batches of data into a Delta table\n",
    "\n",
    "def process_batch(df, batch_id, table_name):\n",
    "    # Add a batch ID column to identify the batch being processed\n",
    "    df = df.withColumn(\"batch_id\", lit(batch_id))\n",
    "    # Add a timestamp column to record the data ingestion time\n",
    "    df = df.withColumn(\"ingest_datetime\", current_timestamp())\n",
    "    # Write the DataFrame to a Delta table in append mode\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "order_items_schema = StructType([\n",
    "    StructField(\"ORDER_ID\", IntegerType(), False),                  # Unique identifier for each order\n",
    "    StructField(\"LINE_ITEM_ID\", IntegerType(), False),              # Unique identifier for each line item in the order\n",
    "    StructField(\"PRODUCT_ID\", IntegerType(), False),                # Identifier for the product in the line item\n",
    "    StructField(\"UNIT_PRICE\", DoubleType(), True),                  # Unit price of the product\n",
    "    StructField(\"QUANTITY\", IntegerType(), True)                    # Quantity of the product ordered\n",
    "])\n",
    "\n",
    "# Specify table name, data source location, and checkpoint directory for \"order items\" data\n",
    "order_items_table_name = 'stoyan.bronze_order_items'\n",
    "order_items_load_location = 's3://data-engineering-upskill-final-exam/stoyan/input_data/order_items'\n",
    "order_items_checkpoint_location = 's3://data-engineering-upskill-final-exam/stoyan/bronze_order_items/checkpoint'\n",
    "\n",
    "# Define a streaming DataFrame for reading \"order items\" data\n",
    "order_items_stream = (\n",
    "spark.readStream\n",
    ".format(\"cloudFiles\")                                               # Enable Auto Loader for cloud file sources\n",
    ".option(\"cloudFiles.format\", \"csv\")                                 # Specify the input file format as CSV\n",
    ".option(\"header\", \"true\")                                           # Indicate that the first row contains column headers\n",
    ".schema(order_items_schema)                                         # Apply the defined schema for order items\n",
    ".load(order_items_load_location)                                    # Load data from the specified S3 location \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a347a1-23a2-45ec-a49d-c800fad78339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x7fcf44dc30d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the stream to a Delta table with batch processing\n",
    "\n",
    "order_items_stream.writeStream \\\n",
    ".foreachBatch(lambda df, batch_id: process_batch(df, batch_id, order_items_table_name)) \\\n",
    ".option(\"checkpointLocation\", order_items_checkpoint_location) \\\n",
    ".trigger(availableNow=True) \\\n",
    ".outputMode(\"append\") \\\n",
    ".start() "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_order_item",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}